# -*- coding: utf-8 -*-
"""DLI_Hosanna.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15sduaU8YbhuTQCPeUwDjvGBVN_cG2o46
"""

!pip install shap

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    precision_score,
    recall_score,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)
from sklearn.utils import resample

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# For SHAP feature selection (install with: !pip install shap)
import shap

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# =============================================================================
# 1. DATA LOADING AND INITIAL EXPLORATION
# =============================================================================

print("Loading datasets...")
Dataset1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Normal_data.csv')
Dataset2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/OVS.csv')
Dataset3 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/metasploitable-2.csv')

# Combine datasets
df = pd.concat([Dataset1, Dataset2, Dataset3], axis=0, ignore_index=True)
print(f"Combined dataset shape: {df.shape}")

# Define column names (assuming standard flow-based features)
feature_columns = [
    'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts',
    'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean',
    'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',
    'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std',
    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std',
    'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std',
    'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',
    'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',
    'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var',
    'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt',
    'URG Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',
    'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts',
    'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',
    'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',
    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'
]

# Adjust columns based on actual dataset structure
if df.shape[1] == len(feature_columns):
    df.columns = feature_columns
else:
    print(f"Column count mismatch. Dataset has {df.shape[1]} columns, expected {len(feature_columns)}")
    # Use existing column names if available, otherwise create generic ones
    if 'Label' not in df.columns:
        df.columns = [f'feature_{i}' for i in range(df.shape[1]-1)] + ['Label']

print(f"Dataset columns: {list(df.columns)}")

# =============================================================================
# 2. EXPLORATORY DATA ANALYSIS (EDA)
# =============================================================================

print("\n=== EXPLORATORY DATA ANALYSIS ===")

# Basic info
print(f"Dataset shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# Check for missing values
print(f"\nMissing values per column:")
missing_values = df.isnull().sum()
if missing_values.sum() > 0:
    print(missing_values[missing_values > 0])
else:
    print("No missing values found")

# Check data types
print(f"\nData types:")
print(df.dtypes.value_counts())

# Label distribution
print(f"\nLabel distribution:")
label_counts = df['Label'].value_counts()
print(label_counts)
print(f"\nLabel percentages:")
print((label_counts / len(df) * 100).round(2))

# Visualize label distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
label_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
plt.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%')
plt.title('Class Distribution (Percentage)')

plt.tight_layout()
plt.show()

# Statistical summary of numerical features
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
if 'Label' in numeric_columns:
    numeric_columns.remove('Label')

print(f"\nStatistical summary of numerical features:")
print(df[numeric_columns].describe())

# =============================================================================
# 3. DATA PREPROCESSING
# =============================================================================

print("\n=== DATA PREPROCESSING ===")

# Remove duplicates
initial_size = len(df)
df = df.drop_duplicates().reset_index(drop=True)
print(f"Removed {initial_size - len(df)} duplicate rows")

# Handle infinite values
df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna().reset_index(drop=True)
print(f"Dataset shape after removing NaN/Inf: {df.shape}")

# Encode labels
label_encoder = LabelEncoder()
df['Label_encoded'] = label_encoder.fit_transform(df['Label'])
print(f"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}")

# Separate features and target
X = df[numeric_columns].values
y = df['Label_encoded'].values
num_classes = len(np.unique(y))

print(f"Feature matrix shape: {X.shape}")
print(f"Target vector shape: {y.shape}")
print(f"Number of classes: {num_classes}")

# Handle class imbalance (optional - downsample majority class)
def balance_dataset(X, y, method='downsample'):
    df_temp = pd.DataFrame(X)
    df_temp['target'] = y

    if method == 'downsample':
        # Find minority class size
        min_class_size = df_temp['target'].value_counts().min()

        # Downsample each class to minority size
        balanced_dfs = []
        for class_label in df_temp['target'].unique():
            class_df = df_temp[df_temp['target'] == class_label]
            if len(class_df) > min_class_size:
                class_df = resample(class_df,
                                  replace=False,
                                  n_samples=min_class_size,
                                  random_state=42)
            balanced_dfs.append(class_df)

        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        return balanced_df.drop('target', axis=1).values, balanced_df['target'].values

    return X, y

# Optional: Balance dataset (uncomment if needed)
# X, y = balance_dataset(X, y)
# print(f"Balanced dataset shape: {X.shape}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert labels to categorical for multi-class classification
y_train_cat = to_categorical(y_train, num_classes)
y_test_cat = to_categorical(y_test, num_classes)

print(f"Training set: {X_train_scaled.shape}, {y_train_cat.shape}")
print(f"Test set: {X_test_scaled.shape}, {y_test_cat.shape}")

# =============================================================================
# 4. SHAP FEATURE SELECTION (Simplified version)
# =============================================================================

print("\n=== FEATURE SELECTION ===")

# Train a simple model for SHAP analysis
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train_scaled, y_train)

# Get feature importances as a proxy for SHAP (since SHAP can be computationally expensive)
feature_importance = rf_model.feature_importances_
feature_names = [f'feature_{i}' for i in range(len(feature_importance))]

# Sort features by importance
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("Top 20 most important features:")
print(importance_df.head(20))

# Select top features (e.g., top 30)
top_features_idx = importance_df.head(30).index
X_train_selected = X_train_scaled[:, top_features_idx]
X_test_selected = X_test_scaled[:, top_features_idx]

print(f"Selected features shape: {X_train_selected.shape}")

# =============================================================================
# 5. CNN-MLP HYBRID MODEL ARCHITECTURE
# =============================================================================

print("\n=== BUILDING HYBRID CNN-MLP MODEL ===")

def create_hybrid_model(input_shape, num_classes):
    # Input layer
    input_layer = Input(shape=(input_shape,))
    from tensorflow.keras.layers import Reshape
    reshaped_input = Reshape((input_shape, 1))(input_layer)

    # CNN branch
    conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(reshaped_input)
    pool1 = MaxPooling1D(pool_size=2)(conv1)
    conv2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(pool1)
    pool2 = MaxPooling1D(pool_size=2)(conv2)

    # Flatten CNN output
    cnn_flattened = Flatten()(pool2)
    cnn_dense = Dense(128, activation='relu')(cnn_flattened)
    cnn_dropout = Dropout(0.3)(cnn_dense)

    # MLP branch
    mlp_dense1 = Dense(128, activation='relu')(input_layer)
    mlp_dropout1 = Dropout(0.3)(mlp_dense1)
    mlp_dense2 = Dense(64, activation='relu')(mlp_dropout1)
    mlp_dropout2 = Dropout(0.3)(mlp_dense2)

    # Combine CNN and MLP branches
    combined = concatenate([cnn_dropout, mlp_dropout2])

    # Final layers
    final_dense = Dense(64, activation='relu')(combined)
    final_dropout = Dropout(0.3)(final_dense)

    # Output layer
    if num_classes == 2:
        output = Dense(1, activation='sigmoid')(final_dropout)
    else:
        output = Dense(num_classes, activation='softmax')(final_dropout)

    model = Model(inputs=input_layer, outputs=output)
    return model

# Create the model
input_shape = X_train_selected.shape[1]
model = create_hybrid_model(input_shape, num_classes)

# Compile the model
if num_classes == 2:
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    # For binary classification, adjust target format
    y_train_final = y_train
    y_test_final = y_test
else:
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    y_train_final = y_train_cat
    y_test_final = y_test_cat

# Model summary
model.summary()

# =============================================================================
# 6. MODEL TRAINING
# =============================================================================

print("\n=== TRAINING THE MODEL ===")

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=0.0001
)

# Train the model
history = model.fit(
    X_train_selected, y_train_final,
    batch_size=32,
    epochs=100,
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

#model.save('/content/drive/My Drive/Colab Notebooks/model_cnn.h5')

# =============================================================================
# 7. MODEL EVALUATION
# =============================================================================

print("\n=== MODEL EVALUATION ===")

# Make predictions
if num_classes == 2:
    y_pred_prob = model.predict(X_test_selected).flatten()
    y_pred = (y_pred_prob > 0.5).astype(int)
else:
    y_pred_prob = model.predict(X_test_selected)
    y_pred = np.argmax(y_pred_prob, axis=1)
    y_test_final = np.argmax(y_test_final, axis=1)

# Calculate metrics
accuracy = accuracy_score(y_test_final, y_pred)
precision = precision_score(y_test_final, y_pred, average='weighted')
recall = recall_score(y_test_final, y_pred, average='weighted')
f1 = f1_score(y_test_final, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# ROC AUC (for binary classification or multiclass with OvR)
try:
    if num_classes == 2:
        roc_auc = roc_auc_score(y_test_final, y_pred_prob)
    else:
        roc_auc = roc_auc_score(y_test_final, y_pred_prob, multi_class='ovr')
    print(f"ROC AUC: {roc_auc:.4f}")
except:
    print("ROC AUC: Could not calculate")
    roc_auc = None

# Confusion Matrix
cm = confusion_matrix(y_test_final, y_pred)
print(f"\nConfusion Matrix:")
print(cm)

# =============================================================================
# 8. VISUALIZATIONS
# =============================================================================

print("\n=== CREATING VISUALIZATIONS ===")

# Training history
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Accuracy
axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')
axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')
axes[0, 0].set_title('Model Accuracy')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Loss
axes[0, 1].plot(history.history['loss'], label='Train Loss')
axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')
axes[0, 1].set_title('Model Loss')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Confusion Matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
axes[1, 0].set_title('Confusion Matrix')
axes[1, 0].set_xlabel('Predicted')
axes[1, 0].set_ylabel('Actual')

# ROC Curve (if available)
if roc_auc is not None and num_classes == 2:
    fpr, tpr, _ = roc_curve(y_test_final, y_pred_prob)
    axes[1, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random')
    axes[1, 1].set_xlabel('False Positive Rate')
    axes[1, 1].set_ylabel('True Positive Rate')
    axes[1, 1].set_title('ROC Curve')
    axes[1, 1].legend()
else:
    axes[1, 1].text(0.5, 0.5, 'ROC Curve\nNot Available\nfor Multiclass',
                    ha='center', va='center', transform=axes[1, 1].transAxes)
    axes[1, 1].set_title('ROC Curve')

plt.tight_layout()
plt.show()

# Feature importance plot
plt.figure(figsize=(12, 8))
top_20_features = importance_df.head(20)
plt.barh(range(len(top_20_features)), top_20_features['importance'])
plt.yticks(range(len(top_20_features)), top_20_features['feature'])
plt.xlabel('Feature Importance')
plt.title('Top 20 Most Important Features')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

#=============================================================================
# 9. DETAILED CLASSIFICATION REPORT
# =============================================================================

print("\n=== CLASSIFICATION REPORT ===")
print(classification_report(y_test_final, y_pred))

# =============================================================================
# 10. RESULTS SUMMARY
# =============================================================================

print("\n=== FINAL RESULTS SUMMARY ===")
print("="*50)
print(f"Dataset: InSDN (Combined)")
print(f"Total samples: {len(df):,}")
print(f"Features used: {input_shape}")
print(f"Classes: {num_classes}")
print("="*50)
print("PERFORMANCE METRICS:")
print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")
if roc_auc is not None:
    print(f"ROC AUC:   {roc_auc:.4f}")
print("="*50)

# Compare with paper results
print("\nCOMPARISON WITH PAPER RESULTS:")
print("Paper reported InSDN accuracy: 99.98%")
print(f"Our model accuracy: {accuracy*100:.2f}%")
if accuracy >= 0.99:
    print(" Achieved high accuracy comparable to paper")
elif accuracy >= 0.95:
    print(" Achieved good accuracy (>95%)")
else:
    print("Accuracy below expected range - consider hyperparameter tuning")